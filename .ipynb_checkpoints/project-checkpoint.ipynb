{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6d54f-dae5-43b3-ad40-b7b201bd17d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d2238-35cd-4998-bb8f-7c907af6818c",
   "metadata": {},
   "source": [
    "# **MARKET-BASKET ANALYSIS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d58f04-d2e4-4e30-b041-e04f5ca4abeb",
   "metadata": {},
   "source": [
    "## Massive Algorithm - Data Science for Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30d303-7b2d-4f77-9f60-7dfda5adc634",
   "metadata": {},
   "source": [
    "**Angelica Longo, Melissa Rizzi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f691-8d9b-44be-84c5-4e58d70505f3",
   "metadata": {},
   "source": [
    "The goal of this project is to implement a system for **detecting frequent itemsets**, commonly known as **market-basket analysis**.\n",
    "In this notebook, the detector treats each user’s reviewed books as a basket, with books serving as items.\n",
    "\n",
    "The project is based on the **[Amazon Books Review](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews)** dataset, published on Kaggle under the public domain CC0 license. Data is downloaded during the execution of the scripts via an API and contains variables related to users and their reviews of purchased books.\n",
    "\n",
    "Given the large volume of data (3 million rows), a reasonable subsample is created using **PySpark**, consisting of approximately 500'000 rows, while ensuring scalability for the full dataset.\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "- **Preprocessing** – This phase includes data cleaning, checking data integrity, handling null values, removing duplicates, and computing the overall mean to verify consistency with the selected subsample.\n",
    "- **Subsampling** – A subset of data is created while maintaining a representative distribution of user choices and ratings.\n",
    "- **Frequent Itemset Mining** – The final step involves implementing an algorithm to identify frequent itemsets within the dataset.\n",
    "\n",
    "This structured approach ensures both **efficiency** and **scalability** while maintaining **data integrity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731e154-e05f-47b4-8401-fbf00ac84f77",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "- [1. Data Import](#1-Data-Import)\n",
    "- [2. Data PreProcessing](#2-data-preprocessing)\n",
    "  - [2.1 Data Integrity](#21-data-integrity)\n",
    "  - [2.2 Missing Data](#22-missing-data)\n",
    "  - [2.3 Data Duplicates](#22-data-duplicates)\n",
    "  - [2.4 Rating Means](#22-rating-means)\n",
    "- [3. Subsample Creation](#3-subsample-creation)\n",
    "  - [3.1 Sample Reliability](#31-sample-realibility)\n",
    "- [4. Algorithm Implementation](#4-algorithm-implementation)\n",
    "  - [4.1 A-priori Algorithms](#41-a-priori-algorithms)\n",
    "    - [4.1.1 Mlxtend](#411-mlxtend)\n",
    "    - [4.1.2 From scratch](#411-from-scratch)\n",
    "  - [4.2 SON Algorithm](#42-son-algorithm)\n",
    "- [5. Extend on Larger Datasets](#5-extend-on-larger-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12e9da-54f1-4dd0-bfbc-9682eaae02f1",
   "metadata": {},
   "source": [
    "---\n",
    "## **1. Data Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2f9470-ba56-462f-9829-388832253f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec451398-739e-49be-8a9a-e76f18bdaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = \"xxxxxx\"\n",
    "os.environ['KAGGLE_KEY'] = \"xxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d6b5c9-0837-4d38-a26c-7afc7acdf129",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download --unzip mohamedbakhet/amazon-books-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f666223-9b9b-48fb-b264-9e895cddc5d4",
   "metadata": {},
   "source": [
    "---\n",
    "## **2. Data PreProcessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8516ec6a-2fc2-45db-8a19-29787ca5437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, sum, when, collect_set,count\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fedb1a1-5a11-482a-982a-e2b4794efee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"MassiveData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4170ba-faf9-427d-8cd6-37d6e11af3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
      "|        Id|               Title|Price|       User_id|         profileName|review/helpfulness|review/score|review/time|      review/summary|         review/text|\n",
      "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
      "|1882931173|Its Only Art If I...| NULL| AVCGYZL8FQQTD|\"Jim of Oz \"\"jim-...|               7/7|         4.0|  940636800|Nice collection o...|This is only for ...|\n",
      "|0826414346|Dr. Seuss: Americ...| NULL|A30TK6U7DNS82R|       Kevin Killian|             10/10|         5.0| 1095724800|   Really Enjoyed It|I don't care much...|\n",
      "|0826414346|Dr. Seuss: Americ...| NULL|A3UH4UZ4RSVO82|        John Granger|             10/11|         5.0| 1078790400|Essential for eve...|\"If people become...|\n",
      "|0826414346|Dr. Seuss: Americ...| NULL|A2MVUWT453QH61|\"Roy E. Perry \"\"a...|               7/7|         4.0| 1090713600|Phlip Nel gives s...|Theodore Seuss Ge...|\n",
      "|0826414346|Dr. Seuss: Americ...| NULL|A22X4XUPKF66MR|\"D. H. Richards \"...|               3/3|         4.0| 1107993600|Good academic ove...|\"Philip Nel - Dr....|\n",
      "+----------+--------------------+-----+--------------+--------------------+------------------+------------+-----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "data = spark.read.csv(\"books_rating.csv\", header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dbc2d99-b6d1-4afb-93f9-30a2b6d53703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 3000000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Rows: {data.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89825f6b-49b7-441f-99f4-b6c2e8312289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-----+\n",
      "|        Id|               Title|       User_id|score|\n",
      "+----------+--------------------+--------------+-----+\n",
      "|1882931173|Its Only Art If I...| AVCGYZL8FQQTD|  4.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A30TK6U7DNS82R|  5.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A3UH4UZ4RSVO82|  5.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A2MVUWT453QH61|  4.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A22X4XUPKF66MR|  4.0|\n",
      "+----------+--------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select only useful columns\n",
    "df = data.select(\"Id\", 'Title', \"User_id\", \"review/score\").withColumnRenamed(\"review/score\", \"score\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a5990-8e66-46ba-b901-e2512c7588e7",
   "metadata": {},
   "source": [
    "### **2.1 Data Integrity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aba7801-26bb-4ede-b4e0-9d8b86872faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b338f85d-630b-4d4f-8bc8-2c0d7bc843c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform 'score' variable in double type\n",
    "df = df.withColumn(\"score\", col(\"score\").cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f19f6eb-d07a-488c-a051-daec53e43aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|min_score| max_score|\n",
      "+---------+----------+\n",
      "|      1.0|1.295568E9|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check score range\n",
    "df.select(min(col(\"score\")).alias(\"min_score\"), max(col(\"score\")).alias(\"max_score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55593ded-7fa4-402a-b75c-e813567c3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min_score|max_score|\n",
      "+---------+---------+\n",
      "|      1.0|      5.0|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep just data with the 'score' values in the correct range [1, 5]\n",
    "df = df.filter((col(\"score\") >= 1) & (col(\"score\") <= 5))\n",
    "df.select(min(\"score\").alias(\"min_score\"), max(\"score\").alias(\"max_score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d1899-4dea-41f1-ac47-5a885e93d370",
   "metadata": {},
   "source": [
    "### **2.2 Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696f1305-282a-48ed-823a-eb518e19d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "| Id|Title|User_id|score|\n",
      "+---+-----+-------+-----+\n",
      "|  0|  196| 561492|    0|\n",
      "+---+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null values for each variable\n",
    "null_counts = df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd56cd-47dd-454c-8222-1b69d34c40c0",
   "metadata": {},
   "source": [
    "There are many missing values in the User_id variable. Since our goal is to identify baskets of items purchased by the same user, we ultimately decided to drop the missing values, as we couldn’t identify a suitable method to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "178c2f35-40a7-4a64-9e64-11634befd3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-----+\n",
      "|        Id|               Title|       User_id|score|\n",
      "+----------+--------------------+--------------+-----+\n",
      "|1882931173|Its Only Art If I...| AVCGYZL8FQQTD|  4.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A30TK6U7DNS82R|  5.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A3UH4UZ4RSVO82|  5.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A2MVUWT453QH61|  4.0|\n",
      "|0826414346|Dr. Seuss: Americ...|A22X4XUPKF66MR|  4.0|\n",
      "+----------+--------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove null values\n",
    "df_clean = df.dropna()\n",
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "792236e6-bcae-4999-9c02-3a79a618e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - After cleaning: 2420237\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "n_rows_clean = df_clean.count()\n",
    "print(f\"Number of Rows - After cleaning: {n_rows_clean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "662c1bcb-4598-4f21-8ee2-688ff372086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+\n",
      "| Id|Title|User_id|score|\n",
      "+---+-----+-------+-----+\n",
      "|  0|    0|      0|    0|\n",
      "+---+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_counts = df_clean.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8eca9a-a340-44b9-8721-8f50635c574f",
   "metadata": {},
   "source": [
    "### **2.3 Data Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b58a5aca-1373-48a0-825a-1fc589a19843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - After duplicates removal: 2383199\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated rows\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "\n",
    "n_rows_clean = df_clean.count()\n",
    "print(f\"Number of Rows - After duplicates removal: {n_rows_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a113-16f5-4ab7-a08d-38ebc3257acb",
   "metadata": {},
   "source": [
    "Up until now, we’ve performed a general cleaning of the dataset. From here on, we’ll focus exclusively on the three columns that are relevant to our analysis (Id, User_id, and score), forming a new dataset: df_short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd6df60-8224-4062-8083-f351ae6564c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----+\n",
      "|        Id|       User_id|score|\n",
      "+----------+--------------+-----+\n",
      "|B000879GGE|A2HDZHLMT3L5IO|  5.0|\n",
      "|B000NKGYMK| A7VSVB6Z0JHOV|  2.0|\n",
      "|050552421X|A1HSG6RIH5NHH4|  5.0|\n",
      "|B0007H4QBK|A1I78HZLE3O1SD|  5.0|\n",
      "|0671424793|A1G6A8W57HZZJR|  5.0|\n",
      "+----------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove useless columns\n",
    "df_short = df_clean.select(\"Id\", \"User_id\",\"score\")\n",
    "df_short.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ce2b8f-8fea-4242-bfcd-79b4c2cf08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicated rows for the three considered variables\n",
    "df_short= df_short.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e30303-5bc3-4aa8-a1f4-3b0aeb9a757f",
   "metadata": {},
   "source": [
    "Given that the same user could have rated the same book twice, we want to compute the mean of the different scores given by the same user to the same book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7088e5e5-defb-4e04-a380-8ba5402126d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates considering only 'Id' and 'User_id'\n",
    "duplicati = df_short.groupBy(\"Id\", \"User_id\").count().filter(\"count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4695327d-85fe-4038-a0c8-3692f2dd5524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+----------+\n",
      "|        Id|       User_id|mean_score|\n",
      "+----------+--------------+----------+\n",
      "|0001047604|A1ZQ1LUQ9R6JHZ|       5.0|\n",
      "|0001047655|A12N9YU5K516JF|       4.0|\n",
      "|0001047655|A1EB4FLIXNX0LK|       2.0|\n",
      "|0001047655|A1NS4974T51EU1|       5.0|\n",
      "|0001047655|A2C8IVS3AEH96R|       1.0|\n",
      "+----------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute average score for every (Id, User_id)\n",
    "score_mean = df_short.groupBy('Id', 'User_id').agg(F.mean('score').alias('mean_score'))\n",
    "df_final = df_short.join(score_mean, on=['Id', 'User_id'], how='left')\n",
    "\n",
    "# Creation of the final preprocessed dataset\n",
    "df_final = df_final.select('Id','User_id', 'mean_score')\n",
    "df_final = df_final.dropDuplicates()\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff19bedb-5e69-46f1-9d61-06d2f882e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - Final dataset: 2380155\n"
     ]
    }
   ],
   "source": [
    "n_rows_final = df_final.count()\n",
    "print(f\"Number of Rows - Final dataset: {n_rows_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3346df7-d6eb-4ea7-a798-3065c25ae2e0",
   "metadata": {},
   "source": [
    "Before continuing the analysis, we considered only books that received a score above 3, as low ratings might indicate lack of engagement with the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90c4c9e0-1b0c-45c7-80af-db428b132812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and keep just rows with rating >= 3\n",
    "df_final = df_final.filter(col(\"mean_score\") >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94011b4-e184-4181-9970-403c2d700230",
   "metadata": {},
   "source": [
    "Then we filtered for users who have rated at least 2 books, because otherwise, if they have only one high rating, we wouldn't be able to understand what other books they might buy based on that rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9966280-bf64-4104-9b6d-33689f2ef4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows after all preprocessing steps: 1488780\n"
     ]
    }
   ],
   "source": [
    "# Filter and keep just users who rated > 1 book\n",
    "user_counts = df_final.groupBy(\"User_id\").agg(count(\"Id\").alias(\"book_count\"))\n",
    "users_with_multiple_books = user_counts.filter(col(\"book_count\") > 1).select(\"User_id\")\n",
    "\n",
    "df_final = df_final.join(users_with_multiple_books, on=\"User_id\", how=\"inner\")\n",
    "print(f'Number of Rows after all preprocessing steps: {df_final.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d926afe6-235c-44e0-8763-eba5a1e0b9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|             User_id|        Id|mean_score|\n",
      "+--------------------+----------+----------+\n",
      "|A00274963RTZUW5BU...|B00086Q244|       5.0|\n",
      "|A00274963RTZUW5BU...|B0006Y8M7S|       5.0|\n",
      "|A00540411RKGTDNU5...|B000H9R1Q0|       5.0|\n",
      "|A00540411RKGTDNU5...|B000Q032UY|       5.0|\n",
      "|A00540411RKGTDNU5...|B000GQG7D2|       5.0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433c56b-d689-47f5-9909-fef27f4ccb77",
   "metadata": {},
   "source": [
    "### **2.4 Rating Means**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc304d-8f56-4051-815d-49ff13645eb9",
   "metadata": {},
   "source": [
    "We want to calculate the overall average score to see if consistency is maintained after creating the subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "165372fe-dcaa-47ce-a264-3bbf09c6d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean score - Final dataset: 4.542176703967902\n"
     ]
    }
   ],
   "source": [
    "df_final = df_final.withColumn(\"mean_score\", F.col(\"mean_score\").cast(\"double\"))\n",
    "\n",
    "overall_mean = df_final.agg(F.avg(\"mean_score\")).collect()[0][0]\n",
    "print(f\"Overall mean score - Final dataset: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ea9f6-42b3-4586-ae5a-f06239029d1b",
   "metadata": {},
   "source": [
    "---\n",
    "## **3. Subsample Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21056f28-3e8c-44f4-a835-609c111f028b",
   "metadata": {},
   "source": [
    "We aim to create a subsample that remains consistent with the original dataset. To achieve this, we select a fraction of users while ensuring that all their reviews are included. This approach allows us to better represent their purchasing behavior and rating patterns, preserving the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f2591d-06d3-4f9c-a6ee-06f4103590a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different users: 268637\n"
     ]
    }
   ],
   "source": [
    "num_users = df_final.select(\"User_id\").distinct().count()\n",
    "print(f\"Total number of different users: {num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "122df0f3-2da9-442e-ab50-e641e40e9550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different books: 162125\n"
     ]
    }
   ],
   "source": [
    "print(f'Total number of different books: {df_final.select(\"Id\").distinct().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94cee6f8-b6c5-4a3b-a5f8-4caffdd9f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just 20% of the users\n",
    "sample_fraction = 0.2\n",
    "user_sample = df_final.select(\"User_id\").distinct().sample(fraction=sample_fraction, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b97d58b5-6af5-43c1-9aa7-7f5651c51363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|             User_id|        Id|mean_score|\n",
      "+--------------------+----------+----------+\n",
      "|A07084061WTSSXN6V...|B000Q34B8I|       5.0|\n",
      "|A07084061WTSSXN6V...|0140817751|       5.0|\n",
      "|A07084061WTSSXN6V...|0808510002|       5.0|\n",
      "|A07084061WTSSXN6V...|0613642910|       5.0|\n",
      "|A07084061WTSSXN6V...|B000FC1BYM|       5.0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the subsample with the selected users\n",
    "df_sampled = df_final.join(user_sample, on=\"User_id\", how=\"inner\")\n",
    "df_sampled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f48b9e1-b2f5-4014-91ef-2f1308b39929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - Sample: 295872\n"
     ]
    }
   ],
   "source": [
    "# Check subsample size\n",
    "n_rows_sample = df_sampled.count()\n",
    "print(f\"Number of Rows - Sample: {n_rows_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0869e35a-d2c5-4a99-b25f-dfd385438b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- mean_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data integrity\n",
    "df_sampled.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f3da9-ed42-40c5-986e-e55617e1cf42",
   "metadata": {},
   "source": [
    "### **3.1 Sample reliability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72b5f009-e652-47ac-98e8-7554a553f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books rated by each user - Original: 5.54\n",
      "Books rated by each user - Subsample: 5.49\n"
     ]
    }
   ],
   "source": [
    "# Check number of books rated by each user\n",
    "avg_rows_original = df_final.groupBy(\"User_id\").count().agg(F.mean(\"count\")).collect()[0][0]\n",
    "avg_rows_sampled = df_sampled.groupBy(\"User_id\").count().agg(F.mean(\"count\")).collect()[0][0]\n",
    "\n",
    "print(f\"Books rated by each user - Original: {avg_rows_original:.2f}\")\n",
    "print(f\"Books rated by each user - Subsample: {avg_rows_sampled:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e469486-b73f-41ec-88f7-8e1e3149233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Mean and Standard Deviation\n",
      "+-----------------+------------------+\n",
      "|  avg(mean_score)|stddev(mean_score)|\n",
      "+-----------------+------------------+\n",
      "|4.542176703967902| 0.679797550978736|\n",
      "+-----------------+------------------+\n",
      "\n",
      "Sample Mean and Standard Deviation\n",
      "+----------------+------------------+\n",
      "| avg(mean_score)|stddev(mean_score)|\n",
      "+----------------+------------------+\n",
      "|4.53571026209532|0.6831053333812903|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Score Average and Standard Deviation\n",
    "print('Original Mean and Standard Deviation')\n",
    "df_final.select(F.mean(\"mean_score\"), F.stddev(\"mean_score\")).show()\n",
    "print('Sample Mean and Standard Deviation')\n",
    "df_sampled.select(F.mean(\"mean_score\"), F.stddev(\"mean_score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4bba1-1e78-4823-9144-166150aa1e88",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. Algorithms Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9feefa45-bd56-4fce-85dd-4a8fdc66363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose whether to work with a subsample or the entire dataset\n",
    "def use_sample(is_subsample):\n",
    "    if is_subsample:\n",
    "        data = df_sampled\n",
    "    else:\n",
    "        data = df_final\n",
    "    print(f'Dataset Size: {data.count()}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f5712e3-9a79-43b7-8a6d-dae95eae8776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 295872\n"
     ]
    }
   ],
   "source": [
    "# In algorithms implementation the used dataset is df_filtered\n",
    "df_filtered = use_sample(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9da92c-5008-44b9-b306-b48c9c9e264c",
   "metadata": {},
   "source": [
    "### **4.1 A-priori Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431e324-cd3c-4622-8536-53e2ba7706a2",
   "metadata": {},
   "source": [
    "The Apriori algorithm is an association rule mining method used to discover frequent patterns within large datasets. It works by first identifying the most frequently occurring itemsets and then extracting association rules that express relationships between these itemsets. The algorithm follows an iterative approach, progressively eliminating less frequent itemsets, improving efficiency. It is commonly used in market basket analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603fb44-ea46-46ad-8489-9911e1352245",
   "metadata": {},
   "source": [
    "#### **4.1.1 Mlxtend**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c28b9-25cf-4397-9572-cdbb4b027305",
   "metadata": {},
   "source": [
    "In the context of Apriori, MLxtend provides an easy-to-use implementation for association rule mining. The apriori function in MLxtend helps identify frequent itemsets from transaction data, while the association_rules function generates association rules based on these frequent itemsets. It is widely used for tasks like market basket analysis, where the goal is to find associations between products frequently bought together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6c54af1-3f80-4a41-91f7-941fa000b878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.functions import collect_set\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4cff28e8-d21d-4c02-91b3-3341ac8e9c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             User_id|               books|\n",
      "+--------------------+--------------------+\n",
      "|A07084061WTSSXN6V...|[0808510002, B000...|\n",
      "|      A100NGGXRQF0AQ|[B000U424PU, 1568...|\n",
      "|      A100TQ7ZRE0W02|[0971237034, 0976...|\n",
      "|A1019451ZPRJ0N3JO06M|[B000N2HBZM, 0001...|\n",
      "|      A1033RWNZWEMR5|[B0006FD9NE, B000...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group items for every user\n",
    "transactions = df_filtered.groupBy(\"User_id\").agg(collect_set(\"Id\").alias(\"books\"))\n",
    "transactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e90433d-5637-4512-a63a-f06a596bb65f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_id</th>\n",
       "      <th>books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A07084061WTSSXN6VLV92</td>\n",
       "      <td>[0808510002, B000Q34B8I, 0521639522, B000FC1BY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A100NGGXRQF0AQ</td>\n",
       "      <td>[B000U424PU, 1568521332, 0066620694, 158160032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A100TQ7ZRE0W02</td>\n",
       "      <td>[0971237034, 0976325608, 0971237018, 0972800522]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1019451ZPRJ0N3JO06M</td>\n",
       "      <td>[B000N2HBZM, 0001050087, 0786197382, 0786192550]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1033RWNZWEMR5</td>\n",
       "      <td>[B0006FD9NE, B0006AEPTG, B000IY4THI]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 User_id                                              books\n",
       "0  A07084061WTSSXN6VLV92  [0808510002, B000Q34B8I, 0521639522, B000FC1BY...\n",
       "1         A100NGGXRQF0AQ  [B000U424PU, 1568521332, 0066620694, 158160032...\n",
       "2         A100TQ7ZRE0W02   [0971237034, 0976325608, 0971237018, 0972800522]\n",
       "3   A1019451ZPRJ0N3JO06M   [B000N2HBZM, 0001050087, 0786197382, 0786192550]\n",
       "4         A1033RWNZWEMR5               [B0006FD9NE, B0006AEPTG, B000IY4THI]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a Pandas dataframe\n",
    "pandas_df = transactions.toPandas()\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "327e06d9-f4a4-4bc0-9f46-769ccd463169",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_books = set(item for sublist in pandas_df[\"books\"] for item in sublist)\n",
    "book_index = {book: idx for idx, book in enumerate(unique_books)}  \n",
    "sparse_matrix = lil_matrix((len(pandas_df), len(unique_books)), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed5667fd-3693-4768-8934-b50ea5676b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\melis\\AppData\\Local\\Temp\\ipykernel_2676\\3139448467.py:7: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  encoded_df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=book_index.keys())\n"
     ]
    }
   ],
   "source": [
    "for row_idx, books in enumerate(pandas_df[\"books\"]):\n",
    "    for book in books:\n",
    "        col_idx = book_index[book]\n",
    "        sparse_matrix[row_idx, col_idx] = 1\n",
    "        \n",
    "# Sparse Matrix\n",
    "encoded_df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix, columns=book_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b3b7fb-b101-47e7-9b5d-19c6138250b9",
   "metadata": {},
   "source": [
    "- Function **apriori** included in Mlxtend library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6b7fb644-c8aa-4c7e-a1e4-2daed9ab3909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chosen support is 1,2% \n",
    "frequent_itemsets = apriori(encoded_df, min_support=0.012, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b3492f3-6484-40ae-9ed9-489dcd2a9560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 frequent itemsets:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.012980</td>\n",
       "      <td>(B000NWU3I4, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.012869</td>\n",
       "      <td>(B000PC54NG, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.012850</td>\n",
       "      <td>(B000NWU3I4, B000PC54NG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.012850</td>\n",
       "      <td>(B000NWU3I4, B000PC54NG, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.012832</td>\n",
       "      <td>(B000NWQXBA, B000PC54NG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.012832</td>\n",
       "      <td>(B000NWQXBA, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.012832</td>\n",
       "      <td>(B000NWQXBA, B000PC54NG, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>(B000NWU3I4, B000NWQXBA)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>(B000NWU3I4, B000NWQXBA, B000PC54NG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>(B000NWU3I4, B000NWQXBA, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.012813</td>\n",
       "      <td>(B000NWU3I4, B000NWQXBA, B000PC54NG, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.012758</td>\n",
       "      <td>(B000H9R1Q0, B000PC54NG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.012758</td>\n",
       "      <td>(B000Q032UY, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.012758</td>\n",
       "      <td>(B000H9R1Q0, B000ILIJE0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.012758</td>\n",
       "      <td>(B000H9R1Q0, B000ILIJE0, B000PC54NG)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                          itemsets\n",
       "42   0.012980                          (B000NWU3I4, B000ILIJE0)\n",
       "38   0.012869                          (B000PC54NG, B000ILIJE0)\n",
       "39   0.012850                          (B000NWU3I4, B000PC54NG)\n",
       "107  0.012850              (B000NWU3I4, B000PC54NG, B000ILIJE0)\n",
       "19   0.012832                          (B000NWQXBA, B000PC54NG)\n",
       "21   0.012832                          (B000NWQXBA, B000ILIJE0)\n",
       "74   0.012832              (B000NWQXBA, B000PC54NG, B000ILIJE0)\n",
       "22   0.012813                          (B000NWU3I4, B000NWQXBA)\n",
       "75   0.012813              (B000NWU3I4, B000NWQXBA, B000PC54NG)\n",
       "78   0.012813              (B000NWU3I4, B000NWQXBA, B000ILIJE0)\n",
       "157  0.012813  (B000NWU3I4, B000NWQXBA, B000PC54NG, B000ILIJE0)\n",
       "29   0.012758                          (B000H9R1Q0, B000PC54NG)\n",
       "40   0.012758                          (B000Q032UY, B000ILIJE0)\n",
       "31   0.012758                          (B000H9R1Q0, B000ILIJE0)\n",
       "94   0.012758              (B000H9R1Q0, B000ILIJE0, B000PC54NG)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep itemsets with at least 2 books\n",
    "filtered_itemsets = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) >= 2)]\n",
    "\n",
    "# Descending order\n",
    "sorted_itemsets = filtered_itemsets.sort_values(by='support', ascending=False)\n",
    "sorted_itemsets.head(15)\n",
    "\n",
    "print(\"Top 15 frequent itemsets:\")\n",
    "sorted_itemsets.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78583795-12e6-4db0-a5e8-d5793df9a9d8",
   "metadata": {},
   "source": [
    "- Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f5c2702-95e7-47e9-b1eb-fe11aad10031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Association Rules:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>(B000NWU3I4)</td>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>(B000NWU3I4)</td>\n",
       "      <td>0.012980</td>\n",
       "      <td>0.997151</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.988604</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>(B000NWU3I4)</td>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>76.928991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>(B000NWU3I4)</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>76.928991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>(B000NWU3I4, B000PC54NG)</td>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>(B000NWU3I4, B000ILIJE0)</td>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>76.928991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>(B000PC54NG, B000ILIJE0)</td>\n",
       "      <td>(B000NWU3I4)</td>\n",
       "      <td>0.012850</td>\n",
       "      <td>0.998559</td>\n",
       "      <td>76.928991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>(B000NWQXBA)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.985755</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>(B000NWQXBA)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.997118</td>\n",
       "      <td>77.706052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>(B000NWQXBA, B000ILIJE0)</td>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.706052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>(B000NWQXBA, B000PC54NG)</td>\n",
       "      <td>(B000ILIJE0)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.820513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>(B000PC54NG, B000ILIJE0)</td>\n",
       "      <td>(B000NWQXBA)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>0.997118</td>\n",
       "      <td>77.706052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(B000NWQXBA)</td>\n",
       "      <td>(B000PC54NG)</td>\n",
       "      <td>0.012832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>77.706052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  antecedents   consequents   support  confidence       lift\n",
       "66               (B000NWU3I4)  (B000ILIJE0)  0.012980    1.000000  76.820513\n",
       "67               (B000ILIJE0)  (B000NWU3I4)  0.012980    0.997151  76.820513\n",
       "58               (B000PC54NG)  (B000ILIJE0)  0.012869    1.000000  76.820513\n",
       "59               (B000ILIJE0)  (B000PC54NG)  0.012869    0.988604  76.820513\n",
       "60               (B000NWU3I4)  (B000PC54NG)  0.012850    0.990000  76.928991\n",
       "61               (B000PC54NG)  (B000NWU3I4)  0.012850    0.998559  76.928991\n",
       "452  (B000NWU3I4, B000PC54NG)  (B000ILIJE0)  0.012850    1.000000  76.820513\n",
       "453  (B000NWU3I4, B000ILIJE0)  (B000PC54NG)  0.012850    0.990000  76.928991\n",
       "454  (B000PC54NG, B000ILIJE0)  (B000NWU3I4)  0.012850    0.998559  76.928991\n",
       "25               (B000ILIJE0)  (B000NWQXBA)  0.012832    0.985755  76.820513\n",
       "21               (B000PC54NG)  (B000NWQXBA)  0.012832    0.997118  77.706052\n",
       "255  (B000NWQXBA, B000ILIJE0)  (B000PC54NG)  0.012832    1.000000  77.706052\n",
       "254  (B000NWQXBA, B000PC54NG)  (B000ILIJE0)  0.012832    1.000000  76.820513\n",
       "256  (B000PC54NG, B000ILIJE0)  (B000NWQXBA)  0.012832    0.997118  77.706052\n",
       "20               (B000NWQXBA)  (B000PC54NG)  0.012832    1.000000  77.706052"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.2)\n",
    "\n",
    "rules_filtrate = rules[rules[\"consequents\"].apply(lambda x: len(x) == 1)]\n",
    "\n",
    "print(\"\\nAssociation Rules:\\n\")\n",
    "rules_filtrate[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values(by='support', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60ec6b-afb6-424f-a691-a10e262ced88",
   "metadata": {},
   "source": [
    "#### **4.1.2 From scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5c52ac6-6841-4e5a-98db-6069738b0382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bab85ac-693b-400a-9d97-328e57c088a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_frequent_books(rdd, threshold):\n",
    "    # Creates pairs of books for each user\n",
    "    pairs_rdd = rdd.flatMap(lambda books: [tuple(sorted(pair)) for pair in combinations(books, 2)])\n",
    "    # Count pairs frequency\n",
    "    pair_counts_rdd = pairs_rdd.map(lambda pair: (pair, 1)).reduceByKey(lambda a, b: a + b)\n",
    "    # Filter pairs where counts >= threshold\n",
    "    frequent_pairs_rdd = pair_counts_rdd.filter(lambda pair_count: pair_count[1] >= threshold)\n",
    "    return frequent_pairs_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e8f72cb-0f18-4597-9cd5-8a72d6be27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert df_filtered as RDD \n",
    "rdd = df_filtered.rdd.map(lambda row: (row[\"User_id\"], row[\"Id\"])) \\\n",
    "                     .groupByKey() \\\n",
    "                     .map(lambda x: list(set(x[1]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44dca566-6be6-456a-89d1-9e0100af6600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Books must be rated together by at least 50 users\n",
    "threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5d1b352-2144-495c-8e8a-6826cc3d2d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a-priori algorithm\n",
    "frequent_book_pairs = apriori_frequent_books(rdd, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20a5904e-be1b-4d02-ae8c-2981d085da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 frequent itemsets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('B000ILIJE0', 'B000NWU3I4'), 710),\n",
       " (('B000NWU3I4', 'B000PC54NG'), 704),\n",
       " (('B000ILIJE0', 'B000PC54NG'), 704),\n",
       " (('B000NWQXBA', 'B000PC54NG'), 702),\n",
       " (('B000NWQXBA', 'B000NWU3I4'), 702),\n",
       " (('B000ILIJE0', 'B000NWQXBA'), 702),\n",
       " (('B000H9R1Q0', 'B000PC54NG'), 696),\n",
       " (('B000NWQXBA', 'B000Q032UY'), 696),\n",
       " (('B000ILIJE0', 'B000Q032UY'), 696),\n",
       " (('B000H9R1Q0', 'B000Q032UY'), 696),\n",
       " (('B000H9R1Q0', 'B000NWU3I4'), 696),\n",
       " (('B000H9R1Q0', 'B000ILIJE0'), 696),\n",
       " (('B000PC54NG', 'B000Q032UY'), 696),\n",
       " (('B000H9R1Q0', 'B000NWQXBA'), 696),\n",
       " (('B000NWU3I4', 'B000Q032UY'), 696)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 15 frequent itemsets:\")\n",
    "frequent_book_pairs.sortBy(lambda x: -x[1]).take(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7c9fd9d-571d-414f-b9ed-7eaf41a0c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_df = frequent_book_pairs.toDF([\"pair\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fbb383ff-d720-4f00-85a3-f3fa408257fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each book appears in the dataset\n",
    "book_counts = rdd.flatMap(lambda books: [(book, 1) for book in books]) \\\n",
    "                 .reduceByKey(lambda a, b: a + b) \\\n",
    "                 .toDF([\"book\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a4bbb10-0b27-4813-aded-dd3d764789c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "|book_B    |book_A    |pair_count|count_A|count_B|support             |confidence_AtoB   |confidence_BtoA   |lift             |\n",
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "|B000NWU3I4|B000ILIJE0|710       |711    |710    |0.013170831246405846|0.9985935302390999|1.0               |75.8185654008439 |\n",
      "|B000PC54NG|B000NWU3I4|704       |710    |704    |0.013059528447140445|0.9915492957746479|1.0               |75.92535211267607|\n",
      "|B000PC54NG|B000ILIJE0|704       |711    |704    |0.013059528447140445|0.9901547116736991|1.0               |75.81856540084388|\n",
      "|B000NWU3I4|B000NWQXBA|702       |702    |710    |0.013022427514051978|1.0               |0.9887323943661972|75.92535211267607|\n",
      "|B000PC54NG|B000NWQXBA|702       |702    |704    |0.013022427514051978|1.0               |0.9971590909090909|76.57244318181819|\n",
      "|B000NWQXBA|B000ILIJE0|702       |711    |702    |0.013022427514051978|0.9873417721518988|1.0               |75.8185654008439 |\n",
      "|B000PC54NG|B000H9R1Q0|696       |696    |704    |0.012911124714786577|1.0               |0.9886363636363636|76.57244318181819|\n",
      "|B000Q032UY|B000NWU3I4|696       |710    |696    |0.012911124714786577|0.9802816901408451|1.0               |75.92535211267607|\n",
      "|B000NWQXBA|B000H9R1Q0|696       |696    |702    |0.012911124714786577|1.0               |0.9914529914529915|76.7905982905983 |\n",
      "|B000ILIJE0|B000H9R1Q0|696       |696    |711    |0.012911124714786577|1.0               |0.9789029535864979|75.81856540084388|\n",
      "|B000NWU3I4|B000H9R1Q0|696       |696    |710    |0.012911124714786577|1.0               |0.9802816901408451|75.92535211267607|\n",
      "|B000Q032UY|B000H9R1Q0|696       |696    |696    |0.012911124714786577|1.0               |1.0               |77.45258620689656|\n",
      "|B000Q032UY|B000NWQXBA|696       |702    |696    |0.012911124714786577|0.9914529914529915|1.0               |76.7905982905983 |\n",
      "|B000Q032UY|B000PC54NG|696       |704    |696    |0.012911124714786577|0.9886363636363636|1.0               |76.57244318181819|\n",
      "|B000Q032UY|B000ILIJE0|696       |711    |696    |0.012911124714786577|0.9789029535864979|1.0               |75.81856540084388|\n",
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute association rules\n",
    "rules_df = frequent_df.select(\n",
    "    col(\"pair\").getField(\"_1\").alias(\"book_A\"),\n",
    "    col(\"pair\").getField(\"_2\").alias(\"book_B\"),\n",
    "    col(\"count\").alias(\"pair_count\")\n",
    ")\n",
    "\n",
    "rules_df = rules_df.join(book_counts.withColumnRenamed(\"book\", \"book_A\"), \"book_A\") \\\n",
    "                   .withColumnRenamed(\"count\", \"count_A\") \\\n",
    "                   .join(book_counts.withColumnRenamed(\"book\", \"book_B\"), \"book_B\") \\\n",
    "                   .withColumnRenamed(\"count\", \"count_B\")\n",
    "\n",
    "rules_df = rules_df.withColumn(\"support\", col(\"pair_count\") / rdd.count()) \\\n",
    "                   .withColumn(\"confidence_AtoB\", col(\"pair_count\") / col(\"count_A\")) \\\n",
    "                   .withColumn(\"confidence_BtoA\", col(\"pair_count\") / col(\"count_B\")) \\\n",
    "                   .withColumn(\"lift\", col(\"confidence_AtoB\") / (col(\"count_B\") / rdd.count()))\n",
    "\n",
    "rules_df = rules_df.orderBy(col(\"pair_count\").desc())\n",
    "\n",
    "rules_df.show(15, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33428c2d-ce1a-4f68-8010-c2b808b49779",
   "metadata": {},
   "source": [
    "### **4.2 SON Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cedf70d2-4993-4818-b6c3-1f27c01d6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c2fb049-4941-430e-a070-ff88cd655ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_baskets = (df_filtered.rdd.\n",
    "                map(lambda row: (row[\"User_id\"], row[\"Id\"])).\n",
    "                groupByKey().\n",
    "                mapValues(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d703e23-b325-402f-b744-2506711e7d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SON_baskets = book_baskets.collect()\n",
    "SON_baskets = [basket for _, basket in SON_baskets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36a2f8c9-6b2b-4eab-9387-e79937bd9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0806e301-61ad-4d71-8a57-9b38069a1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 10 chuncks\n",
    "rdd = spark.sparkContext.parallelize(SON_baskets, numSlices=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "348c8134-ddac-4522-b150-2cae1180669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local and global support\n",
    "s = 50 \n",
    "p = 1 / rdd.getNumPartitions()  \n",
    "local_s = s * p  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8be3364d-aa7e-43be-9b86-9f0cea7bb187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First MapReduce: Find candidates\n",
    "def find_frequent_itemsets(partition):\n",
    "    partition_list = list(partition)  \n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    # creating itemsets\n",
    "    for basket in partition_list:\n",
    "        for item in basket:\n",
    "            counts[frozenset([item])] += 1  \n",
    "        for pair in combinations(basket, 2):  \n",
    "            counts[frozenset(pair)] += 1\n",
    "\n",
    "    # Find locally frequent itemsets\n",
    "    frequent_itemsets = [itemset for itemset, count in counts.items() if count >= local_s]\n",
    "\n",
    "    return [(itemset, 1) for itemset in frequent_itemsets]\n",
    "\n",
    "# First Map\n",
    "candidates_rdd = rdd.mapPartitions(find_frequent_itemsets)\n",
    "\n",
    "# First Reduce\n",
    "candidates = candidates_rdd.distinct().collect()\n",
    "candidates_set = set(candidates)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e53bb898-cadb-45ee-8938-1e5b08e3f57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'B0009K75X6', 'B000GRZI8Q'}), 1),\n",
       " (frozenset({'B00017JJ5E', 'B000K0DB8I'}), 1),\n",
       " (frozenset({'0451513967', 'B000C1X8JC'}), 1),\n",
       " (frozenset({'9562910334', 'B000P3LVZA'}), 1),\n",
       " (frozenset({'B0006BV75A', 'B000EVFCRG'}), 1),\n",
       " (frozenset({'B000CRFW9A', 'B000OVMRLA'}), 1),\n",
       " (frozenset({'0141804459', 'B0006AQ4LI'}), 1),\n",
       " (frozenset({'B000IVDZR6'}), 1),\n",
       " (frozenset({'1578152437'}), 1),\n",
       " (frozenset({'B0001FZGSK'}), 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(candidates_set, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68713792-2705-45e4-a49c-7ab944dad71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Mapreduce: Frequent itemsets frequency\n",
    "\n",
    "def count_candidates(partition):\n",
    "    partition_list = list(partition)\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for basket in partition_list:\n",
    "        for candidate in candidates_set:\n",
    "            if candidate[0].issubset(set(basket)):  \n",
    "                counts[candidate[0]] += 1\n",
    "\n",
    "    return counts.items()\n",
    "\n",
    "# Second Map\n",
    "counts_rdd = rdd.mapPartitions(count_candidates)\n",
    "\n",
    "# Second Reduce\n",
    "frequent_itemsets = counts_rdd.reduceByKey(lambda x, y: x + y) \\\n",
    "                             .filter(lambda x: x[1] >= s) \\\n",
    "                             .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db7bce26-e4b4-481e-b220-1f2f03033c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_itemsets = [itemset for itemset in frequent_itemsets if len(itemset[0]) == 2]\n",
    "sorted_itemsets = sorted(filtered_itemsets, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "412b94e7-ff4d-41fb-8567-1c334601cb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(frozenset({'B000ILIJE0', 'B000NWU3I4'}), 710),\n",
       " (frozenset({'B000ILIJE0', 'B000PC54NG'}), 704),\n",
       " (frozenset({'B000NWU3I4', 'B000PC54NG'}), 704),\n",
       " (frozenset({'B000NWQXBA', 'B000PC54NG'}), 702),\n",
       " (frozenset({'B000NWQXBA', 'B000NWU3I4'}), 702),\n",
       " (frozenset({'B000ILIJE0', 'B000NWQXBA'}), 702),\n",
       " (frozenset({'B000PC54NG', 'B000Q032UY'}), 696),\n",
       " (frozenset({'B000NWQXBA', 'B000Q032UY'}), 696),\n",
       " (frozenset({'B000NWU3I4', 'B000Q032UY'}), 696),\n",
       " (frozenset({'B000H9R1Q0', 'B000ILIJE0'}), 696),\n",
       " (frozenset({'B000H9R1Q0', 'B000PC54NG'}), 696),\n",
       " (frozenset({'B000H9R1Q0', 'B000Q032UY'}), 696),\n",
       " (frozenset({'B000H9R1Q0', 'B000NWU3I4'}), 696),\n",
       " (frozenset({'B000H9R1Q0', 'B000NWQXBA'}), 696),\n",
       " (frozenset({'B000ILIJE0', 'B000Q032UY'}), 696)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_itemsets[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d37d0-ac34-4b56-99d0-8e692725d0d8",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Extend on Larger Datasets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7692a-79d2-4d0f-995d-2b7607a3cd30",
   "metadata": {},
   "source": [
    "Prove algorithms scalability by running A-priori on the original dataset (after pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e053626b-2ac9-44ab-9859-5200d62a6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size: 1488780\n"
     ]
    }
   ],
   "source": [
    "df_filtered = use_sample(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15d8c4d4-860e-487a-9414-ad52deaacf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_filtered.rdd.map(lambda row: (row[\"User_id\"], row[\"Id\"])) \\\n",
    "                     .groupByKey() \\\n",
    "                    .map(lambda x: list(set(x[1]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b57c378b-d19b-41e5-ad34-4f0b4a9ad385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 frequent itemsets:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('B000ILIJE0', 'B000NWU3I4'), 3450),\n",
       " (('B000ILIJE0', 'B000PC54NG'), 3428),\n",
       " (('B000NWQXBA', 'B000PC54NG'), 3424),\n",
       " (('B000ILIJE0', 'B000NWQXBA'), 3424),\n",
       " (('B000NWU3I4', 'B000PC54NG'), 3423),\n",
       " (('B000NWQXBA', 'B000NWU3I4'), 3418),\n",
       " (('B000NWQXBA', 'B000Q032UY'), 3400),\n",
       " (('B000ILIJE0', 'B000Q032UY'), 3400),\n",
       " (('B000PC54NG', 'B000Q032UY'), 3400),\n",
       " (('B000H9R1Q0', 'B000PC54NG'), 3396),\n",
       " (('B000H9R1Q0', 'B000Q032UY'), 3396),\n",
       " (('B000H9R1Q0', 'B000ILIJE0'), 3396),\n",
       " (('B000H9R1Q0', 'B000NWQXBA'), 3396),\n",
       " (('B000NWU3I4', 'B000Q032UY'), 3394),\n",
       " (('B000H9R1Q0', 'B000NWU3I4'), 3390)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_book_pairs = apriori_frequent_books(rdd, threshold)\n",
    "print(\"Top 15 frequent itemsets:\")\n",
    "frequent_book_pairs.sortBy(lambda x: -x[1]).take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed44d9f-1f20-4f89-aab5-d9eb1b870217",
   "metadata": {},
   "source": [
    "The top 15 most frequent items are the same as before, proving algorithms scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bbe131f8-0625-4843-9dc2-b721e00e1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_df = frequent_book_pairs.toDF([\"pair\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a3663128-8e92-48cc-b6be-13c37e2e4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many times each book appears in the dataset\n",
    "book_counts = rdd.flatMap(lambda books: [(book, 1) for book in books]) \\\n",
    "                 .reduceByKey(lambda a, b: a + b) \\\n",
    "                 .toDF([\"book\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e14c8efb-0b91-4647-a73d-f3b31927646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "|book_B    |book_A    |pair_count|count_A|count_B|support             |confidence_AtoB   |confidence_BtoA   |lift             |\n",
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "|B000NWU3I4|B000ILIJE0|3450      |3463   |3451   |0.012842609171484196|0.9962460294542305|0.9997102289191538|77.55101263821967|\n",
      "|B000PC54NG|B000ILIJE0|3428      |3463   |3429   |0.01276071427241966 |0.9898931562229281|0.99970836978711  |77.55086841885644|\n",
      "|B000PC54NG|B000NWQXBA|3424      |3424   |3429   |0.012745824290771562|1.0               |0.9985418489355498|78.34266550014581|\n",
      "|B000NWQXBA|B000ILIJE0|3424      |3463   |3424   |0.012745824290771562|0.9887380883626913|1.0               |77.57349119260756|\n",
      "|B000PC54NG|B000NWU3I4|3423      |3451   |3429   |0.012742101795359537|0.9918864097363083|0.9982502187226596|77.70702521211217|\n",
      "|B000NWU3I4|B000NWQXBA|3418      |3424   |3451   |0.012723489318299416|0.9982476635514018|0.9904375543320777|77.70682630931843|\n",
      "|B000Q032UY|B000NWQXBA|3400      |3424   |3400   |0.012656484400882975|0.9929906542056075|1.0               |78.45706775700936|\n",
      "|B000Q032UY|B000ILIJE0|3400      |3463   |3400   |0.012656484400882975|0.9818076812012706|1.0               |77.57349119260758|\n",
      "|B000Q032UY|B000PC54NG|3400      |3429   |3400   |0.012656484400882975|0.9915427238261884|1.0               |78.34266550014581|\n",
      "|B000NWQXBA|B000H9R1Q0|3396      |3396   |3424   |0.012641594419234879|1.0               |0.991822429906542 |78.45706775700934|\n",
      "|B000PC54NG|B000H9R1Q0|3396      |3396   |3429   |0.012641594419234879|1.0               |0.9903762029746281|78.34266550014581|\n",
      "|B000Q032UY|B000H9R1Q0|3396      |3396   |3400   |0.012641594419234879|1.0               |0.9988235294117647|79.01088235294118|\n",
      "|B000ILIJE0|B000H9R1Q0|3396      |3396   |3463   |0.012641594419234879|1.0               |0.9806526133410338|77.57349119260756|\n",
      "|B000Q032UY|B000NWU3I4|3394      |3451   |3400   |0.01263414942841083 |0.9834830483917705|0.9982352941176471|77.70586343259413|\n",
      "|B000NWU3I4|B000H9R1Q0|3390      |3396   |3451   |0.012619259446762732|0.9982332155477032|0.982323964068386 |77.70570162998793|\n",
      "+----------+----------+----------+-------+-------+--------------------+------------------+------------------+-----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute association rules\n",
    "rules_df = frequent_df.select(\n",
    "    col(\"pair\").getField(\"_1\").alias(\"book_A\"),\n",
    "    col(\"pair\").getField(\"_2\").alias(\"book_B\"),\n",
    "    col(\"count\").alias(\"pair_count\")\n",
    ")\n",
    "\n",
    "rules_df = rules_df.join(book_counts.withColumnRenamed(\"book\", \"book_A\"), \"book_A\") \\\n",
    "                   .withColumnRenamed(\"count\", \"count_A\") \\\n",
    "                   .join(book_counts.withColumnRenamed(\"book\", \"book_B\"), \"book_B\") \\\n",
    "                   .withColumnRenamed(\"count\", \"count_B\")\n",
    "\n",
    "rules_df = rules_df.withColumn(\"support\", col(\"pair_count\") / rdd.count()) \\\n",
    "                   .withColumn(\"confidence_AtoB\", col(\"pair_count\") / col(\"count_A\")) \\\n",
    "                   .withColumn(\"confidence_BtoA\", col(\"pair_count\") / col(\"count_B\")) \\\n",
    "                   .withColumn(\"lift\", col(\"confidence_AtoB\") / (col(\"count_B\") / rdd.count()))\n",
    "\n",
    "rules_df = rules_df.orderBy(col(\"pair_count\").desc())\n",
    "\n",
    "rules_df.show(15, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
