{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7d6d54f-dae5-43b3-ad40-b7b201bd17d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d2238-35cd-4998-bb8f-7c907af6818c",
   "metadata": {},
   "source": [
    "# MARKET-BASKET ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d58f04-d2e4-4e30-b041-e04f5ca4abeb",
   "metadata": {},
   "source": [
    "## Massive Algorithm \n",
    "### Data Science for Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30d303-7b2d-4f77-9f60-7dfda5adc634",
   "metadata": {},
   "source": [
    "##### Angelica Longo, Melissa Rizzi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758f691-8d9b-44be-84c5-4e58d70505f3",
   "metadata": {},
   "source": [
    "The goal of this project is to implement a system for **detecting frequent itemsets**, commonly known as **market-basket analysis**.\n",
    "In this notebook, the detector treats each user’s reviewed books as a basket, with books serving as items.\n",
    "\n",
    "The project is based on the **[Amazon Books Review](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews)** dataset, published on Kaggle under the public domain CC0 license. Data is downloaded during the execution of the scripts via an API and contains variables related to users and their reviews of purchased books.\n",
    "\n",
    "Given the large volume of data (3 million rows), a reasonable subsample is created using **PySpark**, consisting of approximately 500,000 rows, while ensuring scalability for the full dataset.\n",
    "\n",
    "The project is structured as follows:\n",
    "\n",
    "- **Preprocessing** – This phase includes data cleaning, checking data integrity, handling null values, removing duplicates, and computing the overall mean to verify consistency with the selected subsample.\n",
    "- **Subsampling** – A subset of data is created while maintaining a representative distribution of user choices and ratings.\n",
    "- **Frequent Itemset Mining** – The final step involves implementing an algorithm to identify frequent itemsets within the dataset.\n",
    "\n",
    "This structured approach ensures both **efficiency** and **scalability** while maintaining **data integrity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f731e154-e05f-47b4-8401-fbf00ac84f77",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Table of Contents\n",
    "- [1. Data Import](#1-Data-Import)\n",
    "- [2. Data PreProcessing](#2-data-preprocessing)\n",
    "  - [2.1 Data Integrity](#21-data-integrity)\n",
    "  - [2.2 Missing Data](#22-missing-data)\n",
    "  - [2.3 Data Duplicates](#22-data-duplicates)\n",
    "  - [2.4 Rating Means](#22-rating-means)\n",
    "- [3. Subsample Creation](#3-subsample-creation)\n",
    "- [4. Frequent Itemset Mining](#4-frequent-itemset-mining)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12e9da-54f1-4dd0-bfbc-9682eaae02f1",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2f9470-ba56-462f-9829-388832253f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec451398-739e-49be-8a9a-e76f18bdaa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['KAGGLE_USERNAME'] = \"melissarizzi\"\n",
    "#os.environ['KAGGLE_KEY'] = \"3ed913e7329a3117a254e67179c0f8bb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e692ed4-ac60-4982-90dc-48ab6dd3797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d6b5c9-0837-4d38-a26c-7afc7acdf129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kaggle datasets download -d mohamedbakhet/amazon-books-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6576e7-9231-41bf-b56e-eb4c073650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(\"amazon-books-reviews.zip\", 'r') as zip_ref:\n",
    "#    zip_ref.extractall(\"amazon_books_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f666223-9b9b-48fb-b264-9e895cddc5d4",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8516ec6a-2fc2-45db-8a19-29787ca5437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, max, sum, when, collect_set, count\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebb13d1-87bc-4721-bebc-bd3cfefd536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fedb1a1-5a11-482a-982a-e2b4794efee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"MapReduce\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4170ba-faf9-427d-8cd6-37d6e11af3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = spark.read.csv(\"amazon_books_data/Books_rating.csv\", header=True, inferSchema=True)\n",
    "#data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89825f6b-49b7-441f-99f4-b6c2e8312289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only useful columns\n",
    "df = data.select(\"Id\", 'Title', \"User_id\", \"review/score\",'review/text').withColumnRenamed(\"review/score\", \"score\")\n",
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a5990-8e66-46ba-b901-e2512c7588e7",
   "metadata": {},
   "source": [
    "#### 2.1 Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aba7801-26bb-4ede-b4e0-9d8b86872faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- review/text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b338f85d-630b-4d4f-8bc8-2c0d7bc843c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      " |-- review/text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform 'score' variable in double type\n",
    "df = df.withColumn(\"score\", col(\"score\").cast(DoubleType()))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f19f6eb-d07a-488c-a051-daec53e43aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|min_score| max_score|\n",
      "+---------+----------+\n",
      "|      1.0|1.295568E9|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check score range\n",
    "df.select(min(col(\"score\")).alias(\"min_score\"), max(col(\"score\")).alias(\"max_score\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55593ded-7fa4-402a-b75c-e813567c3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min_score|max_score|\n",
      "+---------+---------+\n",
      "|      1.0|      5.0|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep just data with the 'score' values in the correct range [1, 5]\n",
    "df = df.filter((col(\"score\") >= 1) & (col(\"score\") <= 5))\n",
    "df.select(min(\"score\").alias(\"min_score\"), max(\"score\").alias(\"max_score\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d1899-4dea-41f1-ac47-5a885e93d370",
   "metadata": {},
   "source": [
    "#### 2.2 Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "696f1305-282a-48ed-823a-eb518e19d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-----+-----------+\n",
      "| Id|Title|User_id|score|review/text|\n",
      "+---+-----+-------+-----+-----------+\n",
      "|  0|  196| 561492|    0|          9|\n",
      "+---+-----+-------+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count null values for each variable\n",
    "null_counts = df.select(\n",
    "    [sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns]\n",
    ")\n",
    "\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd56cd-47dd-454c-8222-1b69d34c40c0",
   "metadata": {},
   "source": [
    "What stands out right away, especially for the purpose of our analysis, is that there are many missing values in the User_id variable. One possible reason for this could be that users who leave reviews but are not registered don’t have a user ID. Our goal is to identify baskets of items purchased by the same users, but without the user ID, this analysis cannot be conducted. We explored the possibility of using profile names instead, by assigning a dummy ID to users with the same name. However, we were aware that this might not provide accurate results due to potential name duplication. Moreover, there were more missing profile names than missing user IDs, which made this solution unfeasible. After considering our options, we ultimately decided to **drop the missing values**, as we couldn’t identify a suitable method to replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "178c2f35-40a7-4a64-9e64-11634befd3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values\n",
    "df_clean = df.dropna()\n",
    "#df_clean.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792236e6-bcae-4999-9c02-3a79a618e555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - Before cleaning: 2981912\n",
      "Number of Rows - After cleaning: 2420235\n"
     ]
    }
   ],
   "source": [
    "# Check data size\n",
    "n_rows = df.count()\n",
    "n_rows_clean = df_clean.count()\n",
    "print(f\"Number of Rows - Before cleaning: {n_rows}\")\n",
    "print(f\"Number of Rows - After cleaning: {n_rows_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8eca9a-a340-44b9-8721-8f50635c574f",
   "metadata": {},
   "source": [
    "#### 2.3 Data Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b58a5aca-1373-48a0-825a-1fc589a19843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - After duplicates removal: 2398220\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicated rows\n",
    "df_clean = df_clean.dropDuplicates()\n",
    "\n",
    "n_rows_clean = df_clean.count()\n",
    "print(f\"Number of Rows - After duplicates removal: {n_rows_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db8a113-16f5-4ab7-a08d-38ebc3257acb",
   "metadata": {},
   "source": [
    "Up until now, we’ve performed a general cleaning of the dataset. From here on, we’ll focus exclusively on the three columns that are relevant to our analysis (Id, User_id, and score), forming a new dataset: df_short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd6df60-8224-4062-8083-f351ae6564c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless columns\n",
    "df_short = df_clean.select(\"Id\", \"User_id\",\"score\")\n",
    "#df_short.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36ce2b8f-8fea-4242-bfcd-79b4c2cf08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and remove duplicated rows for the three considered variables\n",
    "df_short= df_short.dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e30303-5bc3-4aa8-a1f4-3b0aeb9a757f",
   "metadata": {},
   "source": [
    "Given that the same user could have rated the same book twice, we want to compute the mean of the different scores given by the same user to the same book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7088e5e5-defb-4e04-a380-8ba5402126d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates considering only 'Id' and 'User_id'\n",
    "duplicati = df_short.groupBy(\"Id\", \"User_id\").count().filter(\"count > 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4695327d-85fe-4038-a0c8-3692f2dd5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average score for every (Id, User_id)\n",
    "score_mean = df_short.groupBy('Id', 'User_id').agg(F.mean('score').alias('mean_score'))\n",
    "df_final = df_short.join(score_mean, on=['Id', 'User_id'], how='left')\n",
    "\n",
    "# Creation of the final preprocessed dataset\n",
    "df_final = df_final.select('Id','User_id', 'mean_score')\n",
    "df_final = df_final.dropDuplicates()\n",
    "#df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff19bedb-5e69-46f1-9d61-06d2f882e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - Final dataset: 2380153\n"
     ]
    }
   ],
   "source": [
    "n_rows_final = df_final.count()\n",
    "print(f\"Number of Rows - Final dataset: {n_rows_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8433c56b-d689-47f5-9909-fef27f4ccb77",
   "metadata": {},
   "source": [
    "#### 2.4 Rating Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc304d-8f56-4051-815d-49ff13645eb9",
   "metadata": {},
   "source": [
    "We want to calculate the overall average score to see if consistency is maintained after creating the subsample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d55063-645d-49bf-a292-18707abbb343",
   "metadata": {},
   "source": [
    "- Overall mean score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "165372fe-dcaa-47ce-a264-3bbf09c6d651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean score - Final dataset: 4.22386130919595\n"
     ]
    }
   ],
   "source": [
    "df_final = df_final.withColumn(\"mean_score\", F.col(\"mean_score\").cast(\"double\"))\n",
    "\n",
    "overall_mean = df_final.agg(F.avg(\"mean_score\")).collect()[0][0]\n",
    "print(f\"Overall mean score - Final dataset: {overall_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacaa7a-69df-4f0f-b974-35e01a859238",
   "metadata": {},
   "source": [
    "- Mean score for each item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29ae5ae6-01ef-4f19-a09e-67c65d13c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#score_per_id = df_final.groupBy(\"Id\").agg(F.avg(\"mean_score\").alias(\"avg_score_pre\"))\n",
    "#score_per_id.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1f45fa9-1ac1-4e42-a7d9-61c358cfc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data integrity\n",
    "#score_per_id_above_5 = score_per_id.filter(F.col(\"avg_score_pre\") > 5)\n",
    "#score_per_id_above_5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ea9f6-42b3-4586-ae5a-f06239029d1b",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Subsample Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21056f28-3e8c-44f4-a835-609c111f028b",
   "metadata": {},
   "source": [
    "We aim to create a subsample that remains consistent with the original dataset. To achieve this, we select a fraction of users while ensuring that all their reviews are included. This approach allows us to better represent their purchasing behavior and rating patterns, preserving the integrity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f2591d-06d3-4f9c-a6ee-06f4103590a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of different users - Original dataset: 1004214\n"
     ]
    }
   ],
   "source": [
    "num_users = df_final.select(\"User_id\").distinct().count()\n",
    "print(f\"Total number of different users - Original dataset: {num_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94cee6f8-b6c5-4a3b-a5f8-4caffdd9f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep just 20% of the users\n",
    "sample_fraction = 0.2\n",
    "user_sample = df_final.select(\"User_id\").distinct().sample(fraction=sample_fraction, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b97d58b5-6af5-43c1-9aa7-7f5651c51363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the subsample with the selected users\n",
    "df_sampled = df_final.join(user_sample, on=\"User_id\", how=\"inner\")\n",
    "#df_sampled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f48b9e1-b2f5-4014-91ef-2f1308b39929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows - Sample: 471573\n"
     ]
    }
   ],
   "source": [
    "# Check subsample size\n",
    "n_rows_sample = df_sampled.count()\n",
    "print(f\"Number of Rows - Sample: {n_rows_sample}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0869e35a-d2c5-4a99-b25f-dfd385438b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_id: string (nullable = true)\n",
      " |-- Id: string (nullable = true)\n",
      " |-- mean_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data integrity\n",
    "df_sampled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc93f259-0a98-47bd-99d6-5510a467fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mean - Sample: 4.221354205322752\n"
     ]
    }
   ],
   "source": [
    "# Check mean coherence with the original dataset\n",
    "overall_mean_sample = df_sampled.agg(F.avg(\"mean_score\")).collect()[0][0]\n",
    "print(f\"Overall mean - Sample: {overall_mean_sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3291f-03c3-43fe-9f01-2a27374cf616",
   "metadata": {},
   "source": [
    "The overall mean of the subsample is coherent with the overall mean of the original final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4bba1-1e78-4823-9144-166150aa1e88",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7221f0e-72c4-402b-960d-709c9944073f",
   "metadata": {},
   "source": [
    "#### 4.1 FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6333ecd-fbfb-4250-97cd-3ca9600550d5",
   "metadata": {},
   "source": [
    "We considered only books that received a score above 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2720c41-7770-4fff-8ec0-08c7e8724311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and keep just rows with rating >= 3\n",
    "df_filtered = df_sampled.filter(col(\"mean_score\") >= 3)\n",
    "\n",
    "# Filter and keep just users who rated > 1 book\n",
    "user_counts = df_filtered.groupBy(\"User_id\").agg(count(\"Id\").alias(\"book_count\"))\n",
    "users_with_multiple_books = user_counts.filter(col(\"book_count\") > 1).select(\"User_id\")\n",
    "\n",
    "df_filtered = df_filtered.join(users_with_multiple_books, on=\"User_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a500c2-39a1-4990-801b-cedb6cf85b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+\n",
      "|             User_id|        Id|mean_score|\n",
      "+--------------------+----------+----------+\n",
      "|A0236983QUCQMORABO03|1587888408|       5.0|\n",
      "|A0236983QUCQMORABO03|1587888432|       5.0|\n",
      "|A0236983QUCQMORABO03|1593352077|       5.0|\n",
      "|A025268923L497N34...|B000P0UDX4|       5.0|\n",
      "|A025268923L497N34...|B00005UVH9|       5.0|\n",
      "+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51b03cb-f56a-4a54-9437-fd6551e71513",
   "metadata": {},
   "source": [
    "- Creation od Baskets of items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2037cac7-7b56-4bf0-ba27-00692d940305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baskets of items for every user\n",
    "df_basket = df_filtered.groupBy(\"User_id\").agg(collect_set(\"Id\").alias(\"items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3bc740e4-7234-42d4-acac-db0bb74b57e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             User_id|               items|\n",
      "+--------------------+--------------------+\n",
      "|A0236983QUCQMORABO03|[1587888432, 1587...|\n",
      "|A025268923L497N34...|[B000P0UDX4, B000...|\n",
      "|A07084061WTSSXN6V...|[0808510002, B000...|\n",
      "|      A100Q4BGPV187I|[0743236017, 0743...|\n",
      "|      A100TQ7ZRE0W02|[0971237034, 0976...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_basket.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e8eb0f-6d91-4127-a998-a9567fadf9ca",
   "metadata": {},
   "source": [
    "- Algorithm application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40008658-82a6-409f-9cf1-4234c6fa80a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply FP-Growth\n",
    "fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.2)\n",
    "model = fpGrowth.fit(df_basket)\n",
    "\n",
    "                        # Support: probabilità di acquisto di tutto il basket\n",
    "                        # Confidence: probabilità che se compro un basket compro anche l'altro libro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f6985f1-0c3f-4ffd-ae9b-98634b670524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "+------------------------------------------------+----+\n",
      "|items                                           |freq|\n",
      "+------------------------------------------------+----+\n",
      "|[B000ILIJE0]                                    |689 |\n",
      "|[B000NWU3I4]                                    |687 |\n",
      "|[B000NWU3I4, B000ILIJE0]                        |686 |\n",
      "|[B000PC54NG]                                    |685 |\n",
      "|[B000PC54NG, B000NWU3I4]                        |684 |\n",
      "|[B000PC54NG, B000NWU3I4, B000ILIJE0]            |683 |\n",
      "|[B000PC54NG, B000ILIJE0]                        |684 |\n",
      "|[B000NWQXBA]                                    |683 |\n",
      "|[B000NWQXBA, B000PC54NG]                        |683 |\n",
      "|[B000NWQXBA, B000PC54NG, B000NWU3I4]            |682 |\n",
      "|[B000NWQXBA, B000PC54NG, B000NWU3I4, B000ILIJE0]|682 |\n",
      "|[B000NWQXBA, B000PC54NG, B000ILIJE0]            |683 |\n",
      "|[B000NWQXBA, B000NWU3I4]                        |682 |\n",
      "|[B000NWQXBA, B000NWU3I4, B000ILIJE0]            |682 |\n",
      "|[B000NWQXBA, B000ILIJE0]                        |683 |\n",
      "|[B000Q032UY]                                    |678 |\n",
      "|[B000Q032UY, B000PC54NG]                        |678 |\n",
      "|[B000Q032UY, B000PC54NG, B000NWU3I4]            |677 |\n",
      "|[B000Q032UY, B000PC54NG, B000NWU3I4, B000ILIJE0]|677 |\n",
      "|[B000Q032UY, B000PC54NG, B000ILIJE0]            |678 |\n",
      "+------------------------------------------------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows of Frequent Itemsets: 511\n"
     ]
    }
   ],
   "source": [
    "print(\"Frequent Itemsets:\")\n",
    "model.freqItemsets.show(truncate=False)\n",
    "\n",
    "# Count number of Frequent Itemsets\n",
    "num_freq_itemsets = model.freqItemsets.count()\n",
    "print(f\"Number of rows of Frequent Itemsets: {num_freq_itemsets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d37d0-ac34-4b56-99d0-8e692725d0d8",
   "metadata": {},
   "source": [
    "- Alta confidenza e lift elevato: Se vedi una regola con alta confidenza (vicina a 1.0) e un valore di lift molto alto, significa che c'è una forte correlazione tra gli articoli dell'antecedente e quelli del conseguente. Queste sono regole particolarmente utili per le raccomandazioni di prodotto.\n",
    "\n",
    "- Basso supporto, alta confidenza e lift alto: Anche se il supporto è basso (ad esempio, 1% delle transazioni), un lift elevato e una confidenza vicina a 1.0 indicano che la regola è molto significativa per un numero ridotto di transazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "263048da-7825-4c9c-9a1b-394b300bf962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association Rules:\n",
      "+------------------------------------------------------------------------+------------+------------------+-----------------+--------------------+\n",
      "|antecedent                                                              |consequent  |confidence        |lift             |support             |\n",
      "+------------------------------------------------------------------------+------------+------------------+-----------------+--------------------+\n",
      "|[B000NDSX6C, B000GQG7D2, B000H9R1Q0, B000PC54NG, B000NWU3I4, B000ILIJE0]|[B000GQG5MA]|0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|[B000NDSX6C, B000GQG7D2, B000H9R1Q0, B000PC54NG, B000NWU3I4, B000ILIJE0]|[B000Q032UY]|1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|[B000NDSX6C, B000GQG7D2, B000H9R1Q0, B000PC54NG, B000NWU3I4, B000ILIJE0]|[B000NWQXBA]|1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|[B000GQG7D2, B000Q032UY, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000H9R1Q0]|1.0               |79.57227138643069|0.012159406858202038|\n",
      "|[B000GQG7D2, B000Q032UY, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000NWQXBA]|1.0               |78.98975109809663|0.012159406858202038|\n",
      "|[B000GQG7D2, B000Q032UY, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000GQG5MA]|0.9588414634146342|78.49696047226026|0.011658943466172382|\n",
      "|[B000GQG7D2, B000Q032UY, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000NDSX6C]|0.9329268292682927|78.39782311374516|0.01134383688600556 |\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000H9R1Q0]|0.9939485627836612|79.09074478197422|0.012177942539388323|\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000Q032UY]|0.9939485627836612|79.09074478197422|0.012177942539388323|\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000PC54NG]|1.0               |78.75912408759123|0.012252085264133458|\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000NWU3I4]|0.9984871406959153|78.41103528463556|0.012233549582947173|\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000GQG5MA]|0.9591527987897126|78.52244839864187|0.0117516218721038  |\n",
      "|[B000GQG7D2, B000NWQXBA, B000ILIJE0]                                    |[B000NDSX6C]|0.9319213313161876|78.31332682945221|0.011417979610750695|\n",
      "|[B000NDSX6C, B000GQG7D2, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000GQG5MA]|0.9642276422764228|78.93790789197725|0.010991658943466173|\n",
      "|[B000NDSX6C, B000GQG7D2, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000H9R1Q0]|0.9951219512195122|79.18411396503346|0.01134383688600556 |\n",
      "|[B000NDSX6C, B000GQG7D2, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000Q032UY]|0.9951219512195122|79.18411396503346|0.01134383688600556 |\n",
      "|[B000NDSX6C, B000GQG7D2, B000PC54NG, B000NWU3I4, B000ILIJE0]            |[B000NWQXBA]|1.0               |78.98975109809663|0.011399443929564412|\n",
      "|[B000GQG5MA, B000ILIJE0]                                                |[B000PC54NG]|0.9954407294832827|78.4000399352162 |0.012140871177015755|\n",
      "|[B000GQG5MA, B000ILIJE0]                                                |[B000H9R1Q0]|0.986322188449848 |78.48389685378953|0.012029657089898053|\n",
      "|[B000GQG5MA, B000ILIJE0]                                                |[B000Q032UY]|0.986322188449848 |78.48389685378953|0.012029657089898053|\n",
      "+------------------------------------------------------------------------+------------+------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows of Association Rules: 2295\n"
     ]
    }
   ],
   "source": [
    "print(\"Association Rules:\")\n",
    "model.associationRules.show(truncate=False)\n",
    "\n",
    "# Count number of Association Rules\n",
    "num_association_rules = model.associationRules.count()\n",
    "print(f\"Number of rows of Association Rules: {num_association_rules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaa35f8-a718-4ce8-b20f-1881b1f3bac1",
   "metadata": {},
   "source": [
    "- Considering association rules with antecedent and precedent a single book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3be86e41-b31d-458d-8943-34a186b24621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------------------+-----------------+--------------------+\n",
      "|antecedent_single|consequent_single|confidence        |lift             |support             |\n",
      "+-----------------+-----------------+------------------+-----------------+--------------------+\n",
      "|B000NDSX6C       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000GQG7D2       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000H9R1Q0       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000PC54NG       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000NWU3I4       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000ILIJE0       |B000GQG5MA       |0.9640522875816994|78.92355222311484|0.010936051899907321|\n",
      "|B000NDSX6C       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000GQG7D2       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000H9R1Q0       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000PC54NG       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000NWU3I4       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000ILIJE0       |B000Q032UY       |1.0               |79.57227138643069|0.01134383688600556 |\n",
      "|B000NDSX6C       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000GQG7D2       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000H9R1Q0       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000PC54NG       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000NWU3I4       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000ILIJE0       |B000NWQXBA       |1.0               |78.98975109809663|0.01134383688600556 |\n",
      "|B000GQG7D2       |B000H9R1Q0       |1.0               |79.57227138643069|0.012159406858202038|\n",
      "|B000Q032UY       |B000H9R1Q0       |1.0               |79.57227138643069|0.012159406858202038|\n",
      "+-----------------+-----------------+------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "association_rules = model.associationRules\n",
    "\n",
    "association_rules_single = association_rules.withColumn(\n",
    "    \"antecedent_single\", \n",
    "    F.explode(association_rules.antecedent)\n",
    ").withColumn(\n",
    "    \"consequent_single\", \n",
    "    F.explode(association_rules.consequent)\n",
    ")\n",
    "\n",
    "association_rules_single.select(\"antecedent_single\", \"consequent_single\", \"confidence\", \"lift\", \"support\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a193e9b-8840-4921-8671-13a69b99ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7bb7228-f863-4f7f-9350-3cfb5bf21e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A PRIORI ALGORITHMS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c77c1bef-2857-49eb-ad65-a571af2502ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 322.0 failed 1 times, most recent failure: Lost task 1.0 in stage 322.0 (TID 1216) (192.168.1.51 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Creiamo una lista di item frequenti\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m frequent_items_list \u001b[38;5;241m=\u001b[39m \u001b[43mfrequent_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Calcoliamo le combinazioni di item\u001b[39;00m\n\u001b[0;32m     18\u001b[0m item_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(combinations(frequent_items_list, \u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 322.0 failed 1 times, most recent failure: Lost task 1.0 in stage 322.0 (TID 1216) (192.168.1.51 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "# 1. Esplodiamo la colonna \"items_list\" (sostituendo \"items_list\" con il nome effettivo della tua colonna)\n",
    "df_exploded = df_basket.select(F.explode(\"items\").alias(\"item\"))\n",
    "\n",
    "# 2. Calcoliamo la frequenza di ciascun item (frequente itemset singolo)\n",
    "item_counts = df_exploded.groupBy(\"item\").count()\n",
    "\n",
    "# 3. Filtriamo gli item con frequenza >= supporto minimo\n",
    "total_transactions = df_filtered.count()  # Numero totale di transazioni\n",
    "frequent_items = item_counts.filter(item_counts[\"count\"] / total_transactions >= min_support)\n",
    "\n",
    "# 4. Ora generiamo le combinazioni degli item frequenti (pairwise)\n",
    "from itertools import combinations\n",
    "\n",
    "# Creiamo una lista di item frequenti\n",
    "frequent_items_list = frequent_items.select(\"item\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Calcoliamo le combinazioni di item\n",
    "item_pairs = list(combinations(frequent_items_list, 2))\n",
    "\n",
    "# 5. Calcoliamo la frequenza delle combinazioni\n",
    "# Creiamo una colonna con tutte le combinazioni per ogni transazione\n",
    "df_pairs = df_exploded.withColumn(\"pair\", F.array(*[F.when(F.col(\"item\") == item, F.lit(1)).otherwise(0) for item in frequent_items_list]))\n",
    "\n",
    "# Ora possiamo contare le combinazioni e calcolare il supporto per ogni coppia\n",
    "pair_counts = df_pairs.groupBy(\"pair\").agg(F.sum(\"pair\").alias(\"count\"))\n",
    "\n",
    "# 6. Filtriamo le coppie che non raggiungono il supporto minimo\n",
    "frequent_pairs = pair_counts.filter(pair_counts[\"count\"] / total_transactions >= min_support)\n",
    "\n",
    "# Mostra i risultati\n",
    "frequent_items.show()\n",
    "frequent_pairs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b15238-4e42-46fe-a1e8-1fd68d2f8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generare item singoli e calcolare il supporto\n",
    "item_counts = transactions.flatMap(lambda t: [(item, 1) for item in t]) \\\n",
    "                          .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Filtrare per supporto minimo\n",
    "frequent_items = item_counts.filter(lambda x: x[1] / total_transactions >= min_support) \\\n",
    "                            .map(lambda x: x[0]) \\\n",
    "                            .collect()\n",
    "\n",
    "# Creare itemset frequenti\n",
    "frequent_itemsets = []\n",
    "k = 1\n",
    "current_itemsets = [(item,) for item in frequent_items]  # Convertire in tuple\n",
    "\n",
    "while current_itemsets:\n",
    "    # Generare i candidati di livello k\n",
    "    candidates = list(combinations(frequent_items, k))\n",
    "\n",
    "    # Calcolare il supporto per ogni combinazione\n",
    "    candidate_counts = transactions.flatMap(lambda t: [(c, 1) for c in candidates if set(c).issubset(set(t))]) \\\n",
    "                                   .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    # Filtrare per supporto minimo\n",
    "    current_itemsets = candidate_counts.filter(lambda x: x[1] / total_transactions >= min_support) \\\n",
    "                                       .map(lambda x: x[0]) \\\n",
    "                                       .collect()\n",
    "\n",
    "    # Aggiungere agli itemset frequenti\n",
    "    frequent_itemsets.extend(current_itemsets)\n",
    "\n",
    "    # Incrementare k per il prossimo ciclo\n",
    "    k += 1\n",
    "\n",
    "print(\"Frequent Itemsets:\", frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50752f-b5ce-4958-9dc0-a7bd1872704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fe55c-ad36-4726-a2e7-f56fe91c085c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4983363f-3b36-43a8-8e1d-4c93c4911ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733a7fb-2037-4df0-ad0e-cd56875d4613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d680b-9a02-49b0-8bf0-665728bba839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a2599-1da6-41fd-93bb-a7dbf6d28848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a2f35-c051-4451-aa9f-a7b983846c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d84fe8-c99a-4957-bef0-e4a1822192aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7f784f5-285c-4f32-be05-e780e396b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Parametri\n",
    "min_support = 0.01  # Soglia di supporto\n",
    "min_confidence = 0.2  # Soglia di confidenza\n",
    "\n",
    "def apriori(df, min_support):\n",
    "    transactions = df.rdd.map(lambda x: x[1])  # Estrarre transazioni\n",
    "    total_transactions = transactions.count()\n",
    "\n",
    "    # Generare item singoli e calcolare il supporto\n",
    "    item_counts = transactions.flatMap(lambda t: [(item, 1) for item in t]) \\\n",
    "                              .reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    # Filtrare per supporto minimo\n",
    "    frequent_items = item_counts.filter(lambda x: x[1] / total_transactions >= min_support) \\\n",
    "                                .map(lambda x: x[0]) \\\n",
    "                                .collect()\n",
    "\n",
    "    # Creare itemset frequenti\n",
    "    frequent_itemsets = []\n",
    "    k = 1\n",
    "    current_itemsets = [(item,) for item in frequent_items]  # Convertire in tuple\n",
    "\n",
    "    while current_itemsets:\n",
    "        # Generare i candidati di livello k\n",
    "        candidates = list(combinations(frequent_items, k))\n",
    "\n",
    "        # Calcolare il supporto per ogni combinazione\n",
    "        candidate_counts = transactions.flatMap(lambda t: [(c, 1) for c in candidates if set(c).issubset(set(t))]) \\\n",
    "                                       .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "        # Filtrare per supporto minimo\n",
    "        current_itemsets = candidate_counts.filter(lambda x: x[1] / total_transactions >= min_support) \\\n",
    "                                           .map(lambda x: x[0]) \\\n",
    "                                           .collect()\n",
    "\n",
    "        # Aggiungere agli itemset frequenti\n",
    "        frequent_itemsets.extend(current_itemsets)\n",
    "\n",
    "        # Incrementare k per il prossimo ciclo\n",
    "        k += 1\n",
    "\n",
    "    return frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c962abf8-adc0-4d37-b949-3126782d22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per calcolare regole di associazione\n",
    "def generate_rules(frequent_itemsets, df, min_confidence):\n",
    "    transactions = df.rdd.map(lambda x: x[1])\n",
    "    total_transactions = transactions.count()\n",
    "    rules = []\n",
    "\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) > 1:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    consequent = tuple(set(itemset) - set(antecedent))\n",
    "\n",
    "                    # Calcolare supporto\n",
    "                    support_itemset = transactions.filter(lambda t: set(itemset).issubset(set(t))).count() / total_transactions\n",
    "                    support_antecedent = transactions.filter(lambda t: set(antecedent).issubset(set(t))).count() / total_transactions\n",
    "                    confidence = support_itemset / support_antecedent if support_antecedent > 0 else 0\n",
    "\n",
    "                    # Controllare la soglia di confidenza\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence))\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a466a828-7dde-475a-96e6-6bc038c16627",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 268.0 failed 1 times, most recent failure: Lost task 5.0 in stage 268.0 (TID 1032) (192.168.1.51 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Eseguire Apriori\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m frequent_itemsets_ap \u001b[38;5;241m=\u001b[39m \u001b[43mapriori\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_support\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequent Itemsets:\u001b[39m\u001b[38;5;124m\"\u001b[39m, frequent_itemsets_ap)\n",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m, in \u001b[0;36mapriori\u001b[1;34m(df, min_support)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapriori\u001b[39m(df, min_support):\n\u001b[0;32m      8\u001b[0m     transactions \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Estrarre transazioni\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     total_transactions \u001b[38;5;241m=\u001b[39m \u001b[43mtransactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Generare item singoli e calcolare il supporto\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     item_counts \u001b[38;5;241m=\u001b[39m transactions\u001b[38;5;241m.\u001b[39mflatMap(\u001b[38;5;28;01mlambda\u001b[39;00m t: [(item, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m t]) \\\n\u001b[0;32m     13\u001b[0m                               \u001b[38;5;241m.\u001b[39mreduceByKey(\u001b[38;5;28;01mlambda\u001b[39;00m x, y: x \u001b[38;5;241m+\u001b[39m y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2316\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2298\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   2315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2291\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[0;32m   2293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:2044\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2039\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[0;32m   2041\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2044\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2045\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 268.0 failed 1 times, most recent failure: Lost task 5.0 in stage 268.0 (TID 1032) (192.168.1.51 executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:612)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:789)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:397)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "# Eseguire Apriori\n",
    "frequent_itemsets_ap = apriori(df_filtered, min_support)\n",
    "print(\"Frequent Itemsets:\", frequent_itemsets_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7680827b-89bc-4707-b7b1-22c5ff19d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python Path:\", sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7610181-9bc7-4c84-a037-192e1eeaac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generare regole di associazione\n",
    "association_rules_ap = generate_rules(frequent_itemsets_ap, df_filtered, min_confidence)\n",
    "print(\"Association Rules:\")\n",
    "for rule in association_rules_ap:\n",
    "    print(f\"{rule[0]} → {rule[1]}, Confidenza: {rule[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c9383f-1de3-4246-b1f2-84d3151ff38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aacbd0-790a-4f8a-b402-baf78e84e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SON algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241f395-5fd2-4186-a6e4-a748d2f48c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametri\n",
    "total_transactions = df.count()  # Numero totale di transazioni\n",
    "chunk_size = 2  # Numero di partizioni (simula i blocchi)\n",
    "\n",
    "# Funzione per estrarre itemset frequenti localmente (fase 1)\n",
    "def local_frequent_itemsets(partition, min_support, chunk_size):\n",
    "    partition = list(partition)  # Convertire in lista (per evitare problemi con RDD)\n",
    "    partition_size = len(partition)\n",
    "\n",
    "    # Se la partizione è vuota, restituire nulla\n",
    "    if partition_size == 0:\n",
    "        return []\n",
    "\n",
    "    # Calcolare supporto locale\n",
    "    local_threshold = (partition_size / total_transactions) * min_support\n",
    "\n",
    "    # Estrarre item unici\n",
    "    items = [item for transaction in partition for item in transaction[1]]\n",
    "    unique_items = set(items)\n",
    "\n",
    "    # Contare item singoli\n",
    "    item_counts = {item: items.count(item) for item in unique_items}\n",
    "\n",
    "    # Filtrare per supporto minimo\n",
    "    frequent_items = {k: v for k, v in item_counts.items() if v >= local_threshold}\n",
    "\n",
    "    # Generare itemset più grandi\n",
    "    k = 2\n",
    "    current_itemsets = [(item,) for item in frequent_items]\n",
    "    all_frequent_itemsets = set(frequent_items.keys())\n",
    "\n",
    "    while current_itemsets:\n",
    "        candidates = list(combinations(all_frequent_itemsets, k))\n",
    "\n",
    "        # Contare le occorrenze nei blocchi locali\n",
    "        candidate_counts = {c: sum(1 for transaction in partition if set(c).issubset(transaction[1])) for c in candidates}\n",
    "\n",
    "        # Filtrare itemset frequenti locali\n",
    "        current_itemsets = {k: v for k, v in candidate_counts.items() if v >= local_threshold}\n",
    "\n",
    "        # Aggiungere agli itemset frequenti\n",
    "        all_frequent_itemsets.update(current_itemsets.keys())\n",
    "        k += 1\n",
    "\n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# Funzione per filtrare itemset frequenti globalmente (fase 2)\n",
    "def global_filter(itemsets, df, min_support):\n",
    "    transactions = df.rdd.map(lambda x: x[1])\n",
    "    total_transactions = transactions.count()\n",
    "\n",
    "    # Calcolare il supporto globale\n",
    "    global_counts = {itemset: transactions.filter(lambda t: set(itemset).issubset(set(t))).count() for itemset in itemsets}\n",
    "    return {k: v for k, v in global_counts.items() if v / total_transactions >= min_support}\n",
    "\n",
    "# Eseguire SON Algorithm\n",
    "rdd = df.rdd.repartition(chunk_size)\n",
    "local_frequent_sets = rdd.mapPartitions(lambda partition: local_frequent_itemsets(partition, min_support, chunk_size)).distinct()\n",
    "global_frequent_sets = global_filter(local_frequent_sets.collect(), df, min_support)\n",
    "\n",
    "# Stampare i risultati\n",
    "print(\"Itemset frequenti globali:\")\n",
    "for itemset, count in global_frequent_sets.items():\n",
    "    print(f\"{itemset}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
